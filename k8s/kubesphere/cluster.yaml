apiVersion: kubekey.kubesphere.io/v1alpha1
kind: Cluster
metadata:
  name: ks-cluster-chengdu1
spec:
  hosts:
  - {name: master01, address: 192.168.100.91, internalAddress: 192.168.100.91, privateKeyPath: "~/.ssh/id_rsa"} # password-less login with SSH keys
  - {name: master02, address: 192.168.100.92, internalAddress: 192.168.100.92, privateKeyPath: "~/.ssh/id_rsa"} # password-less login with SSH keys
  - {name: master03, address: 192.168.100.93, internalAddress: 192.168.100.93, privateKeyPath: "~/.ssh/id_rsa"} # password-less login with SSH keys
  - {name: node01, address: 192.168.100.1, internalAddress: 192.168.100.1, privateKeyPath: "~/.ssh/id_rsa"} # password-less login with SSH keys
  - {name: node02, address: 192.168.100.2, internalAddress: 192.168.100.2, privateKeyPath: "~/.ssh/id_rsa"} # password-less login with SSH keys
  - {name: node03, address: 192.168.100.3, internalAddress: 192.168.100.3, privateKeyPath: "~/.ssh/id_rsa"} # password-less login with SSH keys
  - {name: node04, address: 192.168.100.4, internalAddress: 192.168.100.4, privateKeyPath: "~/.ssh/id_rsa"} # password-less login with SSH keys
  - {name: node05, address: 192.168.100.5, internalAddress: 192.168.100.5, privateKeyPath: "~/.ssh/id_rsa"} # password-less login with SSH keys
  roleGroups:
    etcd:
     - master01
     - master02
     - master03
    master:
     - master01
     - master02
     - master03
    worker:
     - node01
     - node02
     - node03
     - node04
     - node05
  controlPlaneEndpoint:
    domain: lb.kubesphere.local
    address: "192.168.100.99"
    port: "16443"
  kubernetes:
    version: v1.17.9
    imageRepo: kubesphere
    clusterName: cluster.local
    masqueradeAll: false  # masqueradeAll tells kube-proxy to SNAT everything if using the pure iptables proxy mode. [Default: false]
    maxPods: 120  # maxPods is the number of pods that can run on this Kubelet. [Default: 110]
    nodeCidrMaskSize: 24  # internal network node size allocation. This is the size allocated to each node on your network. [Default: 24]
    proxyMode: ipvs  # mode specifies which proxy mode to use. [Default: ipvs]
  network:
    plugin: calico
    calico:
      ipipMode: Always
      vxlanMode: Never
      vethMTU: 1440
    kubePodsCIDR: 10.233.64.0/18
    kubeServiceCIDR: 10.233.0.0/18
  registry:
    registryMirrors: []
    insecureRegistries: []
    privateRegistry: ""
  addons:
  - name: rbd-provisioner
    namespace: kube-system
    sources:
      chart:
        name: rbd-provisioner
        repo: https://charts.kubesphere.io/test
        values:
        # for more values, see https://github.com/kubesphere/helm-charts/tree/master/src/test/rbd-provisioner
        - ceph.mon=192.168.100.61:6789\,192.168.100.62:6789\,192.168.100.63:6789
        - ceph.pool=ceph-k8s
        - ceph.adminId=admin
        - ceph.adminKey=QVFDZUZXdGZYbkx6RlJBQVJPeG5QUUN4ZjRHYW5XdDNaMlk0S3c9PQo=
        - ceph.userId=kubernetes
        - ceph.userKey=QVFCeUc0TmY1TnhVQ3hBQXlaM1NLODAwaGs1c0NBRHhyaTVlWXc9PQo=
        - sc.isDefault=true
---
apiVersion: installer.kubesphere.io/v1alpha1
kind: ClusterConfiguration
metadata:
  name: ks-installer
  namespace: kubesphere-system
  labels:
    version: v3.0.0
spec:
  local_registry: ""
  persistence:
    storageClass: ""
  authentication:
    jwtSecret: ""
  etcd:
    monitoring: true        # Whether to install etcd monitoring dashboard
    endpointIps: 192.168.100.91,192.168.100.92,192.168.100.93  # etcd cluster endpointIps
    port: 2379              # etcd port
    tlsEnable: true
  common:
    mysqlVolumeSize: 20Gi # MySQL PVC size
    minioVolumeSize: 20Gi # Minio PVC size
    etcdVolumeSize: 20Gi  # etcd PVC size
    openldapVolumeSize: 2Gi   # openldap PVC size
    redisVolumSize: 2Gi # Redis PVC size
    es:
      elasticsearchMasterReplicas: 1   # total number of master nodes, it's not allowed to use even number
      elasticsearchDataReplicas: 1     # total number of data nodes
      elasticsearchMasterVolumeSize: 4Gi   # Volume size of Elasticsearch master nodes
      elasticsearchDataVolumeSize: 20Gi    # Volume size of Elasticsearch data nodes
      logMaxAge: 7
      elkPrefix: logstash
      # externalElasticsearchUrl:
      # externalElasticsearchPort:
  console:
    enableMultiLogin: false
    port: 30880
  alerting:
    enabled: false
  auditing:
    enabled: false
  devops:
    enabled: false
    jenkinsMemoryLim: 2Gi      # Jenkins memory limit
    jenkinsMemoryReq: 1500Mi   # Jenkins memory request
    jenkinsVolumeSize: 8Gi     # Jenkins volume size
    jenkinsJavaOpts_Xms: 512m  # The following three fields are JVM parameters
    jenkinsJavaOpts_Xmx: 512m
    jenkinsJavaOpts_MaxRAM: 2g
  events:
    enabled: false
  logging:
    enabled: false
    logsidecarReplicas: 2
  metrics_server:
    enabled: true
  monitoring:                        #
    prometheusReplicas: 1
    prometheusMemoryRequest: 900Mi   # Prometheus request memory
    prometheusVolumeSize: 20Gi       # Prometheus PVC size
    alertmanagerReplicas: 1          # AlertManager Replicas
  multicluster:
    clusterRole: none
  networkpolicy:
    enabled: false
  notification:
    enabled: false
  openpitrix:
    enabled: false
  servicemesh:
    enabled: false
